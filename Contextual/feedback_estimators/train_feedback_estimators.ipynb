{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train feedback estimators using Logistic and Linear Regression\n",
    "\n",
    "This script trains the following feedback estimators on the ACCORD_BPClass_v2.csv dataset\n",
    "\n",
    "* CVDRiskEstr: logistic regression, obtain accuracy arounnd 0.88\n",
    "* SBPEstr: linear regression, poor fitting, R^2 = 0.03\n",
    "* A1CEstr: linear regression, poor fitting, R^2 = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_fea = ['baseline_age', 'female', 'race_whiteother', #'race_black', \n",
    "                'edu_baseline',\n",
    "                'cvd_hx_baseline', 'baseline_BMI', 'cigarett_baseline']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVDRisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (176906, 22)\n",
      "y.shape =  (176906,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tian/opt/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_score =  0.8791865690624912\n",
      "test_score =  0.8792606410039003\n",
      "train_score2 =  0.8646908527692673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tian/opt/anaconda3/envs/tf/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "def train_CVDRisk_estimator(fn, flag):\n",
    "\n",
    "    # flag = 'BP' or 'BG' or 'BPBG'\n",
    "    \n",
    "    # get the features and labels\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    if flag == 'BP':\n",
    "        medclass_cols = ['BMI', 'sbp', 'hba1c', 'TC', 'hdl', \n",
    "                        'bpclass_none', 'Diur', 'ACE', 'Beta-blocker', 'CCB', \n",
    "                        'ARB', 'Alpha-Beta-blocker', 'Alpha-blocker', 'Sympath', 'Vasod']\n",
    "    elif flag == 'BG':\n",
    "        medclass_cols = ['BMI', 'sbp', 'hba1c', 'TC', 'hdl', \n",
    "                        'bgclass_none', ]\n",
    "    elif flag == 'BPBG':\n",
    "        medclass_cols = ['BMI', 'sbp', 'hba1c', 'TC', 'hdl', \n",
    "                        'bpclass_none', 'Diur', 'ACE', 'Beta-blocker', 'CCB', \n",
    "                        'ARB', 'Alpha-Beta-blocker', 'Alpha-blocker', 'Sympath', 'Vasod',\n",
    "                        'bgclass_none', ]\n",
    "    else:\n",
    "        print('Error: flag must be BP, BG or BPBG')\n",
    "        exit()\n",
    "\n",
    "    fea_cols = context_fea + medclass_cols\n",
    "    # remove the race_black column to avoid multicollinearity, but performance is roughly the same\n",
    "    # fea_cols.remove('race_black')\n",
    "\n",
    "    X = df[fea_cols].values \n",
    "    y = df['CVDRisk_feedback_binary'].values\n",
    "\n",
    "    print('X.shape = ', X.shape)\n",
    "    print('y.shape = ', y.shape)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # Train a logistic regression model to predict the risk of CVD\n",
    "    lr = LogisticRegression(max_iter=400).fit(X_train, y_train)\n",
    "    train_score = lr.score(X_train, y_train)\n",
    "    test_score =  lr.score(X_test, y_test)\n",
    "    print('train_score = ', train_score)\n",
    "    print('test_score = ', test_score)\n",
    "\n",
    "    # retrain the model on the whole dataset\n",
    "    estimator = LogisticRegression(max_iter=200).fit(X, y)\n",
    "    train_score2 = estimator.score(X, y)\n",
    "    print('train_score2 = ', train_score2)\n",
    "\n",
    "    return (estimator, fea_cols)\n",
    "\n",
    "CVDRisk_estimator_BP, CVDRisk_fea_BP = train_CVDRisk_estimator('../../../Codes/Accord/data/ACCORD_BPClass_v2.csv', 'BP')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (176906, 18)\n",
      "y.shape =  (176906,)\n",
      "train_score =  0.03102635614224325\n",
      "test_score =  0.028663873790212402\n",
      "train_score2 =  0.030605337454086223\n"
     ]
    }
   ],
   "source": [
    "# train a linear regression model for SBP feedback\n",
    "def train_SBP_estimator(fn):\n",
    "    # get the features and labels\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    medclass_cols = ['BMI',  \n",
    "                    'bpclass_none', 'Diur', 'ACE', 'Beta-blocker', 'CCB', \n",
    "                    'ARB', 'Alpha-Beta-blocker', 'Alpha-blocker', 'Sympath', 'Vasod']\n",
    "\n",
    "    fea_cols = context_fea + medclass_cols\n",
    "    # remove race_black from fea_cols list, to avoid multicollinearity. But performance is still very poor!\n",
    "    # fea_cols.remove('race_black')\n",
    "\n",
    "    X = df[fea_cols].values \n",
    "    y = df['sbp'].values\n",
    "\n",
    "    print('X.shape = ', X.shape)\n",
    "    print('y.shape = ', y.shape)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # Train a linear regression model to predict the risk of CVD\n",
    "    lr = LinearRegression().fit(X_train, y_train)\n",
    "    train_score = lr.score(X_train, y_train)\n",
    "    test_score =  lr.score(X_test, y_test)\n",
    "    print('train_score = ', train_score)\n",
    "    print('test_score = ', test_score)\n",
    "\n",
    "    # retrain the model on the whole dataset\n",
    "    estimator = LinearRegression().fit(X, y)\n",
    "    train_score2 = estimator.score(X, y)\n",
    "    print('train_score2 = ', train_score2)\n",
    "\n",
    "    return estimator        \n",
    "\n",
    "SBP_estimator = train_SBP_estimator('../../../Codes/Accord/data/ACCORD_BPClass_v2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (290467, 17)\n",
      "y.shape =  (290467,)\n",
      "train_score =  0.09026242567547482\n",
      "test_score =  0.08718707308778972\n",
      "train_score2 =  0.08966819902124046\n"
     ]
    }
   ],
   "source": [
    "# train a linear regression model for A1C feedback\n",
    "def train_A1C_estimator(fn):\n",
    "    # get the features and labels\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    medclass_cols = ['BMI', 'sbp', 'TC', 'hdl',   \n",
    "                    'bgclass_none', 'Bingu', 'Thiaz', 'Sulfon', 'Meglit', \n",
    "                    'Alpha-gluc']\n",
    "\n",
    "    fea_cols = context_fea + medclass_cols\n",
    "    # fea_cols.remove('race_black')\n",
    "    X = df[fea_cols].values \n",
    "    y = df['hba1c'].values\n",
    "\n",
    "    print('X.shape = ', X.shape)\n",
    "    print('y.shape = ', y.shape)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # Train a linear regression model to predict the risk of CVD\n",
    "    lr = LinearRegression().fit(X_train, y_train)\n",
    "    train_score = lr.score(X_train, y_train)\n",
    "    test_score =  lr.score(X_test, y_test)\n",
    "    print('train_score = ', train_score)\n",
    "    print('test_score = ', test_score)\n",
    "\n",
    "    # retrain the model on the whole dataset\n",
    "    estimator = LinearRegression().fit(X, y)\n",
    "    train_score2 = estimator.score(X, y)\n",
    "    print('train_score2 = ', train_score2)\n",
    "\n",
    "    return estimator        \n",
    "\n",
    "SBP_estimator = train_A1C_estimator('../../../Codes/Accord/data/ACCORD_BGClass_v2.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Estimators using ANN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a class which build a ANN model for SBP feedback, using tensorflow framwork\n",
    "   \n",
    "class ANN:\n",
    "    def __init__(self, input_shape, output_shape, hidden_layers, output_activation='linear'):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.hidden_layers = hidden_layers        \n",
    "        \n",
    "        # Define the model architecture\n",
    "        self.model = tf.keras.Sequential()\n",
    "        self.model.add(tf.keras.layers.Input(shape=input_shape))\n",
    "        for i in range(len(hidden_layers)):\n",
    "            self.model.add(tf.keras.layers.Dense(hidden_layers[i], activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(output_shape, activation=output_activation))\n",
    "        \n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=10, batch_size=16):\n",
    "        # Compile the model with appropriate loss function and optimizer\n",
    "        # self.model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        # Evaluate the model on the test set\n",
    "        return self.model.evaluate(x, y)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Use the model to make predictions\n",
    "        return self.model.predict(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVDRisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (176906, 22)\n",
      "y.shape =  (176906,)\n",
      "Epoch 1/20\n",
      "5529/5529 [==============================] - 13s 2ms/step - loss: 0.1450 - val_loss: 0.1126\n",
      "Epoch 2/20\n",
      "5529/5529 [==============================] - 11s 2ms/step - loss: 0.1015 - val_loss: 0.0946\n",
      "Epoch 3/20\n",
      "5529/5529 [==============================] - 11s 2ms/step - loss: 0.0942 - val_loss: 0.0865\n",
      "Epoch 4/20\n",
      "5529/5529 [==============================] - 11s 2ms/step - loss: 0.0903 - val_loss: 0.0822\n",
      "Epoch 5/20\n",
      "5529/5529 [==============================] - 10s 2ms/step - loss: 0.0838 - val_loss: 0.0828\n",
      "Epoch 6/20\n",
      "5529/5529 [==============================] - 20s 4ms/step - loss: 0.0808 - val_loss: 0.0797\n",
      "Epoch 7/20\n",
      "5529/5529 [==============================] - 14s 3ms/step - loss: 0.0774 - val_loss: 0.0851\n",
      "Epoch 8/20\n",
      "5529/5529 [==============================] - 14s 2ms/step - loss: 0.0750 - val_loss: 0.0823\n",
      "Epoch 9/20\n",
      "5529/5529 [==============================] - 11s 2ms/step - loss: 0.0726 - val_loss: 0.0673\n",
      "Epoch 10/20\n",
      "5529/5529 [==============================] - 14s 3ms/step - loss: 0.0706 - val_loss: 0.0717\n",
      "Epoch 11/20\n",
      "5529/5529 [==============================] - 13s 2ms/step - loss: 0.0688 - val_loss: 0.0700\n",
      "Epoch 12/20\n",
      "5529/5529 [==============================] - 11s 2ms/step - loss: 0.0669 - val_loss: 0.0606\n",
      "Epoch 13/20\n",
      "5529/5529 [==============================] - 12s 2ms/step - loss: 0.0655 - val_loss: 0.0696\n",
      "Epoch 14/20\n",
      "5529/5529 [==============================] - 10s 2ms/step - loss: 0.0642 - val_loss: 0.0602\n",
      "Epoch 15/20\n",
      "5529/5529 [==============================] - 15s 3ms/step - loss: 0.0623 - val_loss: 0.0677\n",
      "Epoch 16/20\n",
      "5529/5529 [==============================] - 11s 2ms/step - loss: 0.0614 - val_loss: 0.0559\n",
      "Epoch 17/20\n",
      "5529/5529 [==============================] - 10s 2ms/step - loss: 0.0607 - val_loss: 0.0568\n",
      "Epoch 18/20\n",
      "5529/5529 [==============================] - 10s 2ms/step - loss: 0.0593 - val_loss: 0.0605\n",
      "Epoch 19/20\n",
      "5529/5529 [==============================] - 10s 2ms/step - loss: 0.0588 - val_loss: 0.0583\n",
      "Epoch 20/20\n",
      "5529/5529 [==============================] - 10s 2ms/step - loss: 0.0580 - val_loss: 0.0572\n"
     ]
    }
   ],
   "source": [
    "# train a ANN model for CVDRisk \n",
    "def train_CVDRisk_estimator_ANN(fn, flag, train_onwhole=False):\n",
    "    # flag = 'BP' or 'BG' or 'BPBG'\n",
    "    \n",
    "    # get the features and labels\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    if flag == 'BP':\n",
    "        medclass_cols = ['BMI', 'sbp', 'hba1c', 'TC', 'hdl', \n",
    "                        'bpclass_none', 'Diur', 'ACE', 'Beta-blocker', 'CCB', \n",
    "                        'ARB', 'Alpha-Beta-blocker', 'Alpha-blocker', 'Sympath', 'Vasod']\n",
    "    elif flag == 'BG':\n",
    "        medclass_cols = ['BMI', 'sbp', 'hba1c', 'TC', 'hdl', \n",
    "                        'bgclass_none', ]\n",
    "    elif flag == 'BPBG':\n",
    "        medclass_cols = ['BMI', 'sbp', 'hba1c', 'TC', 'hdl', \n",
    "                        'bpclass_none', 'Diur', 'ACE', 'Beta-blocker', 'CCB', \n",
    "                        'ARB', 'Alpha-Beta-blocker', 'Alpha-blocker', 'Sympath', 'Vasod',\n",
    "                        'bgclass_none', ]\n",
    "    else:\n",
    "        print('Error: flag must be BP, BG or BPBG')\n",
    "        exit()\n",
    "\n",
    "    fea_cols = context_fea + medclass_cols\n",
    "    X = df[fea_cols].values \n",
    "    y = df['CVDRisk_feedback_binary'].values\n",
    "    print('X.shape = ', X.shape)\n",
    "    print('y.shape = ', y.shape)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=100)\n",
    "\n",
    "    model = ANN(X.shape[1], 1, [16, 8], 'sigmoid')\n",
    "    if not train_onwhole: # to explore hyperparameters\n",
    "        model.train(X_train, y_train, X_val, y_val, epochs=20, batch_size=32)\n",
    "        test_score = model.evaluate(X_test, y_test)\n",
    "        print('test_score = ', test_score)\n",
    "        return (None, fea_cols)\n",
    "\n",
    "    else: # retrain the model on the whole dataset\n",
    "        model.train(X, y, X, y, epochs=20, batch_size=32)    \n",
    "        return (model, fea_cols)\n",
    "\n",
    "# none, CVDRisk_fea = train_CVDRisk_estimator_ANN('../../../Codes/Accord/data/ACCORD_BPClass_v2.csv', 'BP')    \n",
    "CVDRisk_estimator_ANN_BP, CVDRisk_fea_BP = train_CVDRisk_estimator_ANN('../../../Codes/Accord/data/ACCORD_BPClass_v2.csv', 'BP', True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (176906, 18)\n",
      "y.shape =  (176906,)\n",
      "Epoch 1/15\n",
      "11057/11057 [==============================] - 23s 2ms/step - loss: 3997356.7500 - val_loss: 305.9937\n",
      "Epoch 2/15\n",
      "11057/11057 [==============================] - 21s 2ms/step - loss: 296.3798 - val_loss: 288.8506\n",
      "Epoch 3/15\n",
      "11057/11057 [==============================] - 22s 2ms/step - loss: 282.4185 - val_loss: 277.2778\n",
      "Epoch 4/15\n",
      "11057/11057 [==============================] - 20s 2ms/step - loss: 268.4738 - val_loss: 262.0031\n",
      "Epoch 5/15\n",
      "11057/11057 [==============================] - 21s 2ms/step - loss: 272.6207 - val_loss: 239.7075\n",
      "Epoch 6/15\n",
      "11057/11057 [==============================] - 19s 2ms/step - loss: 264.8234 - val_loss: 237.6143\n",
      "Epoch 7/15\n",
      "11057/11057 [==============================] - 20s 2ms/step - loss: 231.7049 - val_loss: 227.4745\n",
      "Epoch 8/15\n",
      "11057/11057 [==============================] - 20s 2ms/step - loss: 230.9640 - val_loss: 235.9847\n",
      "Epoch 9/15\n",
      "11057/11057 [==============================] - 22s 2ms/step - loss: 230.8279 - val_loss: 231.6199\n",
      "Epoch 10/15\n",
      "11057/11057 [==============================] - 21s 2ms/step - loss: 230.3996 - val_loss: 229.0944\n",
      "Epoch 11/15\n",
      "11057/11057 [==============================] - 20s 2ms/step - loss: 230.1891 - val_loss: 225.2342\n",
      "Epoch 12/15\n",
      "11057/11057 [==============================] - 19s 2ms/step - loss: 229.6142 - val_loss: 228.5696\n",
      "Epoch 13/15\n",
      "11057/11057 [==============================] - 21s 2ms/step - loss: 229.7500 - val_loss: 225.9502\n",
      "Epoch 14/15\n",
      "11057/11057 [==============================] - 21s 2ms/step - loss: 229.6587 - val_loss: 228.2548\n",
      "Epoch 15/15\n",
      "11057/11057 [==============================] - 20s 2ms/step - loss: 229.1526 - val_loss: 232.5981\n"
     ]
    }
   ],
   "source": [
    "# train SBP estimator using ANN model\n",
    "def train_SBP_estimator_ANN(fn, train_onwhole=False):\n",
    "    # get the features and labels\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    medclass_cols = ['BMI',  \n",
    "                    'bpclass_none', 'Diur', 'ACE', 'Beta-blocker', 'CCB', \n",
    "                    'ARB', 'Alpha-Beta-blocker', 'Alpha-blocker', 'Sympath', 'Vasod']\n",
    "\n",
    "    fea_cols = context_fea + medclass_cols\n",
    "    X = df[fea_cols].values \n",
    "    y = df['sbp'].values\n",
    "\n",
    "    print('X.shape = ', X.shape)\n",
    "    print('y.shape = ', y.shape)\n",
    "    \n",
    "    # Split the data into training, validation and testing sets\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=100)\n",
    "\n",
    "    model = ANN(X.shape[1], 1, [16, 8])\n",
    "    if not train_onwhole: # to explore hyperparameters\n",
    "        model.train(X_train, y_train, X_val, y_val, epochs=20, batch_size=16)\n",
    "        test_score = model.evaluate(X_test, y_test)\n",
    "        print('test_score = ', test_score)\n",
    "        return (None, fea_cols)\n",
    "\n",
    "    else: # retrain the model on the whole dataset\n",
    "        model.train(X, y, X, y, epochs=15, batch_size=16)    \n",
    "        return (model, fea_cols)\n",
    "\n",
    "# none, SBP_fea = train_SBP_estimator_ANN('../../../Codes/Accord/data/ACCORD_BPClass_v2.csv')\n",
    "SBP_estimator_ANN, SBP_fea = train_SBP_estimator_ANN('../../../Codes/Accord/data/ACCORD_BPClass_v2.csv', True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (290467, 17)\n",
      "y.shape =  (290467,)\n",
      "Epoch 1/15\n",
      "9078/9078 [==============================] - 18s 2ms/step - loss: 323339.7812 - val_loss: 16.1397\n",
      "Epoch 2/15\n",
      "9078/9078 [==============================] - 19s 2ms/step - loss: 25794.3047 - val_loss: 920.3678\n",
      "Epoch 3/15\n",
      "9078/9078 [==============================] - 17s 2ms/step - loss: 16736.1738 - val_loss: 5715.7778\n",
      "Epoch 4/15\n",
      "9078/9078 [==============================] - 20s 2ms/step - loss: 14569.7510 - val_loss: 156323.9062\n",
      "Epoch 5/15\n",
      "9078/9078 [==============================] - 25s 3ms/step - loss: 15216.2100 - val_loss: 5436.5928\n",
      "Epoch 6/15\n",
      "9078/9078 [==============================] - 22s 2ms/step - loss: 5136.6772 - val_loss: 1187.3887\n",
      "Epoch 7/15\n",
      "9078/9078 [==============================] - 23s 3ms/step - loss: 1994.5747 - val_loss: 123.3015\n",
      "Epoch 8/15\n",
      "9078/9078 [==============================] - 24s 3ms/step - loss: 5.3624 - val_loss: 1.0676\n",
      "Epoch 9/15\n",
      "9078/9078 [==============================] - 21s 2ms/step - loss: 1.0831 - val_loss: 1.0642\n",
      "Epoch 10/15\n",
      "9078/9078 [==============================] - 19s 2ms/step - loss: 47.7258 - val_loss: 1.0516\n",
      "Epoch 11/15\n",
      "9078/9078 [==============================] - 18s 2ms/step - loss: 1.0664 - val_loss: 1.0554\n",
      "Epoch 12/15\n",
      "9078/9078 [==============================] - 20s 2ms/step - loss: 1.0603 - val_loss: 1.0531\n",
      "Epoch 13/15\n",
      "9078/9078 [==============================] - 15s 2ms/step - loss: 1.0567 - val_loss: 1.0634\n",
      "Epoch 14/15\n",
      "9078/9078 [==============================] - 16s 2ms/step - loss: 1.0939 - val_loss: 1.0377\n",
      "Epoch 15/15\n",
      "9078/9078 [==============================] - 15s 2ms/step - loss: 1.0479 - val_loss: 1.0419\n"
     ]
    }
   ],
   "source": [
    "# train a ANN model for A1C feedback\n",
    "def train_A1C_estimator_ANN(fn, train_onwhole=False):\n",
    "    # get the features and labels\n",
    "    df = pd.read_csv(fn)\n",
    "\n",
    "    medclass_cols = ['BMI', 'sbp', 'TC', 'hdl',   \n",
    "                    'bgclass_none', 'Bingu', 'Thiaz', 'Sulfon', 'Meglit', \n",
    "                    'Alpha-gluc']\n",
    "\n",
    "    fea_cols = context_fea + medclass_cols\n",
    "    X = df[fea_cols].values \n",
    "    y = df['hba1c'].values\n",
    "\n",
    "    print('X.shape = ', X.shape)\n",
    "    print('y.shape = ', y.shape)\n",
    "    \n",
    "    # Split the data into training, validation and testing sets\n",
    "    X_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=100)\n",
    "\n",
    "    model = ANN(X.shape[1], 1, [16, 8])\n",
    "    if not train_onwhole: # to explore hyperparameters\n",
    "        model.train(X_train, y_train, X_val, y_val, epochs=20, batch_size=32)\n",
    "        test_score = model.evaluate(X_test, y_test)\n",
    "        print('test_score = ', test_score)\n",
    "        return (None, fea_cols)\n",
    "    else:\n",
    "        model.train(X, y, X, y, epochs=15, batch_size=32)    \n",
    "        return (model, fea_cols)       \n",
    "\n",
    "# none, A1C_fea = train_A1C_estimator_ANN('../../../Codes/Accord/data/ACCORD_BGClass_v2.csv')\n",
    "A1C_estimator_ANN, A1C_fea = train_A1C_estimator_ANN('../../../Codes/Accord/data/ACCORD_BGClass_v2.csv', True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model for RL feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://ef1c0d68-e16a-48f8-8607-ae7c517c88cc/assets\n",
      "INFO:tensorflow:Assets written to: ram://9afb876a-7a58-40e2-b0f0-0b64595f98d0/assets\n",
      "INFO:tensorflow:Assets written to: ram://20326107-bf74-4531-a314-ed37f7657990/assets\n"
     ]
    }
   ],
   "source": [
    "estimators = {'CVDRisk-BP': CVDRisk_estimator_ANN_BP, 'SBP': SBP_estimator_ANN, 'A1C': A1C_estimator_ANN}\n",
    "feature_cols = {'CVDRisk-BP': CVDRisk_fea_BP, 'SBP': SBP_fea, 'A1C': A1C_fea}\n",
    "\n",
    "# save the estimators and feature columns\n",
    "with open('estimators.pkl', 'wb') as f:\n",
    "    pickle.dump(estimators, f)\n",
    "\n",
    "with open('feature_cols.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_cols, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
